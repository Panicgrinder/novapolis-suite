#!/usr/bin/env python
"""
Sammelt Status-/Metrik-Daten f√ºr docs/TODO.md:
- Neueste Eval-Ergebnisse analysieren (Erfolgsrate, RPG-Anteil, Dauer)
- Verf√ºgbarkeit offener Features pr√ºfen (Caching, Rerun-Failed, Fine-Tune-Skript)
- Optional Markdown-Report schreiben

Aufruf:
  python scripts/todo_gather.py --write-md
"""
from __future__ import annotations
import os, glob, json, argparse
from typing import Any, Dict, List, Optional
from utils.time_utils import now_human

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RESULTS_DIR = os.path.join(PROJECT_ROOT, "eval", "results")
SUMMARIES_DIR = os.path.join(RESULTS_DIR, "summaries")

def latest_results() -> Optional[str]:
    files = sorted(glob.glob(os.path.join(RESULTS_DIR, "results_*.jsonl")))
    return files[-1] if files else None

def parse_results(path: str) -> Dict[str, Any]:
    total = 0
    success = 0
    rpg_mode = 0
    durations: List[int] = []
    failed_ids: List[str] = []
    by_package: Dict[str, Dict[str, Any]] = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                r = json.loads(line)
            except Exception:
                continue
            total += 1
            if r.get("success") is True:
                success += 1
            if r.get("rpg_mode") is True:
                rpg_mode += 1
            d = r.get("duration_ms")
            if isinstance(d, (int, float)):
                durations.append(int(d))
            rid = r.get("id") or r.get("item_id")
            if r.get("success") is not True and rid:
                failed_ids.append(str(rid))
            pkg = r.get("package") or "unknown"
            stats = by_package.setdefault(str(pkg), {"total": 0, "success": 0, "durations": []})
            stats["total"] += 1
            if r.get("success") is True:
                stats["success"] += 1
            if isinstance(d, (int, float)):
                stats["durations"].append(int(d))
    avg_dur = (sum(durations)/len(durations)) if durations else 0.0
    return {
        "path": path,
        "total": total,
        "success": success,
        "success_rate": (success/total*100.0) if total else 0.0,
        "rpg_percentage": (rpg_mode/total*100.0) if total else 0.0,
        "avg_duration_ms": avg_dur,
        "failed_ids": sorted(set(failed_ids)),
        "by_package": by_package,
    }

def file_exists(rel: str) -> bool:
    return os.path.exists(os.path.join(PROJECT_ROOT, rel))

def feature_status() -> Dict[str, Any]:
    # Caching: Datei + Nutzung in map_reduce_summary_llm.py
    caching_file = file_exists("utils/eval_cache.py")
    caching_used = False
    if file_exists("scripts/map_reduce_summary_llm.py"):
        try:
            txt = open(os.path.join(PROJECT_ROOT, "scripts", "map_reduce_summary_llm.py"), "r", encoding="utf-8").read()
            caching_used = ("utils.eval_cache" in txt) or ("cache_llm.jsonl" in txt)
        except Exception:
            pass
    rerun_failed = False  # legacy script replaced by rerun_from_results.py
    fine_tune = file_exists("scripts/fine_tune_pipeline.py")
    curate = file_exists("scripts/curate_dataset_from_latest.py")
    return {
        "caching_available": caching_file,
        "caching_integrated": caching_used,
        "rerun_failed_available": rerun_failed,
        "fine_tune_pipeline_available": fine_tune,
        "curate_dataset_available": curate,
    }

def build_md(results: Optional[Dict[str, Any]], features: Dict[str, Any]) -> str:
    lines: List[str] = []
    lines.append("# TODO-Status ‚Äì Automatischer √úberblick")
    lines.append("")
    lines.append("## Features (offene Punkte)")
    lines.append(f"- Caching/Memoization: {'‚úÖ integriert' if features['caching_integrated'] else ('üü° vorhanden (noch nicht integriert)' if features['caching_available'] else '‚ùå fehlt')}")
    lines.append(f"- Rerun-Failed: ‚úÖ via scripts/rerun_from_results.py")
    lines.append(f"- Fine-Tuning/LoRA Pipeline: {'‚úÖ vorhanden' if features['fine_tune_pipeline_available'] else '‚ùå fehlt'}")
    lines.append(f"- Datensatzkurierung: {'‚úÖ vorhanden' if features['curate_dataset_available'] else '‚ùå fehlt'}")
    lines.append("")
    if results:
        lines.append("## Letzte Eval-Metriken")
        lines.append(f"- Datei: {os.path.relpath(results['path'], PROJECT_ROOT)}")
        lines.append(f"- Tests: {results['success']}/{results['total']} ({results['success_rate']:.1f}%)")
        lines.append(f"- RPG-Anteil: {results['rpg_percentage']:.1f}%")
        lines.append(f"- √ò Dauer: {results['avg_duration_ms']:.0f} ms")
        if results["failed_ids"]:
            lines.append(f"- Fehlgeschlagene IDs: {', '.join(results['failed_ids'][:25])}{' ‚Ä¶' if len(results['failed_ids'])>25 else ''}")
    return "\n".join(lines)


def _write_md_with_frontmatter(out: str, md_body: str, update_text: str = "Generated by todo_gather.py", checks: str = "PASS") -> None:
    """Schreibe eine Markdown-Datei mit exakt formatiertem YAML-Frontmatter und Setext-Headern.

    - Frontmatter-Schl√ºssel in genau dieser Reihenfolge: stand, update, checks
    - stand: ISO-8601 mit Offset (via utils.time_utils.now_iso)
    - danach H1/H2 im Setext-Stil f√ºr die erste H1/H2 im Dokument
    """
    # Erzeuge Frontmatter
    ts = now_human() if now_human is not None else ""

    # Konvertiere nur die Kopfsektion: erste H1 (# ) und die erste H2 (## )
    body = md_body
    # Normalize line endings
    body = body.replace('\r\n', '\n').lstrip('\ufeff')

    # If document starts with an H1 ATX, convert it
    if body.startswith('# '):
        parts = body.split('\n', 1)
        first = parts[0][2:].rstrip()
        rest = parts[1] if len(parts) > 1 else ''
        underline = '=' * len(first)
        body = f"{first}\n{underline}\n\n{rest.lstrip()}"

    # Convert all H2 (ATX '## ') to Setext-style (underline with '-')
    lines = body.split('\n')
    i = 0
    while i < len(lines):
        ln = lines[i]
        if ln.startswith('## '):
            h2 = ln[3:].rstrip()
            lines[i] = h2
            lines.insert(i+1, '-' * len(h2))
            i += 2
        else:
            i += 1
    body = '\n'.join(lines)

    # Ensure single trailing newline at EOF
    if not body.endswith('\n'):
        body = body + '\n'

    os.makedirs(os.path.dirname(out), exist_ok=True)
    with open(out, 'w', encoding='utf-8', newline='\n') as f:
        f.write('---\n')
        f.write(f"stand: {ts}\n")
        f.write(f"update: {update_text}\n")
        f.write(f"checks: {checks}\n")
        f.write('---\n\n')
        f.write(body)

def main(argv: Optional[List[str]] = None) -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--write-md", action="store_true", help="Schreibt Markdown-Report unter eval/results/summaries/")
    ap.add_argument("--out-dir", type=str, default=SUMMARIES_DIR, help="Zielverzeichnis f√ºr Markdown-Ausgabe (f√ºr Dry-Run)")
    args = ap.parse_args(argv)
    latest = latest_results()
    results = parse_results(latest) if latest else None
    features = feature_status()
    md = build_md(results, features)
    print(md)
    if args.write_md:
        out_dir = args.out_dir
        os.makedirs(out_dir, exist_ok=True)
        from utils.time_utils import now_compact
        ts = now_compact()
        out = os.path.join(out_dir, f"todo_status_{ts}.md")
        # Minimal-invasive write via helper to ensure Frontmatter + Setext
        _write_md_with_frontmatter(out, md, update_text="Generated by todo_gather.py", checks="PASS")
        print(f"\nReport gespeichert: {os.path.relpath(out, PROJECT_ROOT)}")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())