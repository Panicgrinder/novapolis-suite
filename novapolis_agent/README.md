---
stand: 2025-11-06 02:11
update: H1/H2 auf Setext-Stil umgestellt (MD003)
checks: keine
---

Novapolis Agent
===============

Ein FastAPI-Backend für einen Conversational Agent innerhalb der Novapolis Suite, der Ollama als LLM verwendet.

Lizenz
------

Dieses Projekt steht unter der MIT-Lizenz. Siehe die Datei `LICENSE` im Repository-Wurzelverzeichnis.

Neuigkeiten (2025-10-20)
------------------------

- Demo→Fantasy: Datensatz-Bezeichnungen vereinheitlicht (`eval-21-40_fantasy_v1.0.*`).
   Maßgeblich sind die Dateien unter `eval/datasets/`.
- Reports: Drei Skripte erzeugen reproduzierbare Berichte unter
   `eval/results/reports/<topic>/<timestamp>/`:
   - `scripts/reports/generate_dependencies_report.py`
   - `scripts/reports/generate_coverage_report.py`
   - `scripts/reports/generate_consistency_report.py`
- CI: Ein Workflow erzeugt die Reports automatisch bei Push und lädt sie als
   Artefakte hoch.
- Legacy-Bereinigung: Unbenutzte Legacy-Endpunkte unter `app/api/endpoints/`
   entfernt; doppelte Exporte in `app/services/__init__.py` bereinigt.

Repository-Info
---------------

- Standard-Branch: `main`
- Optional: Zusätzliche Pyright-Konfig für Skripte: `pyrightconfig.scripts.json`

Einrichtung
----------

1. Python 3.12 installieren
2. Virtuelle Umgebung erstellen und aktivieren:

```powershell
   python -m venv venv
   .\venv\Scripts\activate  # Windows
   source venv/bin/activate  # Linux/Mac
   ```

3. Abhängigkeiten installieren (eine der beiden Varianten):

- Aus dem Repo-Root (empfohlen, bündelt alles):

```powershell
   pip install -r requirements.txt
```

- Direkt im Modul `novapolis_agent` (granular):

```powershell
   # Basis-Laufzeit
   pip install -r requirements/base.txt
   # Dev-Extras (Tests, Lint)
   pip install -r requirements/dev.txt
   # Optional: Trainings-Extras
   pip install -r requirements/train.txt
```

Oder manuell:

```bash
   pip install fastapi uvicorn httpx python-dotenv
   ```

4. Ollama installieren und starten:

```bash
   # Windows-Installer von https://ollama.com/download/windows
   # Nach der Installation:
   ollama serve
   ```

5. LLM-Modell herunterladen:

```powershell
   ollama pull llama3.1:8b
   ```

Anwendung starten
-----------------

```bash
uvicorn app.main:app --reload
```

API-Endpunkte
-------------

- `GET /`: Basis-Endpunkt für Gesundheitsprüfung
- `POST /chat`: Chat-Endpunkt zum Senden von Nachrichten an das LLM

### Chat-Endpunkt verwenden

```bash
curl -X POST http://127.0.0.1:8000/chat \
  -H "Content-Type: application/json" \
  -d "{\"messages\":[{\"role\":\"user\",\"content\":\"Du bist die Chronistin. Stell dich kurz vor.\"}]}"
```

Oder mit PowerShell:

```powershell
Invoke-RestMethod -Uri "http://127.0.0.1:8000/chat" -Method Post -Body '{"messages":[{"role":"user","content":"Du bist die Chronistin. Stell dich kurz vor."}]}' -ContentType "application/json"
```

Swagger-Dokumentation
---------------------

Zugriff auf die API-Dokumentation unter:

```text
http://127.0.0.1:8000/docs
```

Simulation API
--------------

Eine kompakte Simulation steckt in `app/api/sim.py` und stellt einen stetig
fortschreibbaren Weltzustand bereit. Die Endpunkte dienen als leichtgewichtige
Quelle für Visualisierungen oder externe Clients.

### Endpunkte

- `GET /world/state` – liefert `{tick, time, regions, actors, events}` als aktuellen Snapshot.
- `POST /world/step` – erwartet `{dt: float}`, erhöht Tick sowie Zeit und gibt den aktualisierten Zustand zurück.

Der Server liest `AGENT_PORT` (Standard `8765`) aus der Umgebung.

### Start (Windows PowerShell)

1. `.env.example` nach `.env` kopieren oder `AGENT_PORT` manuell setzen.
2. VS Code Task `Run Agent Dev` ausführen **oder**

```powershell
   $port = $env:AGENT_PORT
   if (-not $port) { $port = 8765 }
   uvicorn app.api.sim:app --host 127.0.0.1 --port $port --reload
   ```

3. Probeaufruf:

```powershell
   Invoke-RestMethod -Uri "http://127.0.0.1:$port/world/state" -Method Get
   ```

### Start (Dev Container)

1. Dev-Container öffnen (`Reopen in Container`).
2. Post-Create installiert `requirements.txt` und `requirements-dev.txt` automatisch.
3. Task `Run Agent Dev` startet den Uvicorn-Server im Container; der Port 8765 wird an den Host weitergeleitet.

Einstellungen/Umgebung
----------------------

Konfiguration per `.env` (siehe Beispiele in `app/core/settings.py`). Wichtige Felder:

Hinweis: Bei aktiviertem Rate Limiting wird pro IP innerhalb eines 60s-Fensters begrenzt (in-memory, best-effort).

### LLM-Optionen (Ollama) – Defaults & Overrides

Der Agent unterstützt eine Reihe von Sampling-/Decoding-Optionen. Defaults sind zentral in `app/core/settings.py` hinterlegt und können via `.env` überschrieben werden. Pro Request lassen sich Optionen in `ChatRequest.options` setzen; diese überschreiben die Defaults.

Unterstützte Optionen (Auswahl):

- Temperatur/Sampling: `temperature`, `top_p`, `top_k`, `min_p`, `typical_p`, `tfs_z`
- Länge/Kontext: `num_predict` (Alias: `max_tokens`), `num_ctx`
- Penalties: `repeat_penalty`, `repeat_last_n`, `presence_penalty`, `frequency_penalty`, `penalize_newline`
- Steuerung/Seed: `seed`, `stop` (String oder Liste), `host` (Ollama Base-URL)
- Mirostat: `mirostat` (0/1/2), `mirostat_tau`, `mirostat_eta`

Zentrale Defaults (aus Settings; Beispielwerte):

```ini
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=40
MIN_P=0.0
TYPICAL_P=1.0
TFS_Z=1.0
MIROSTAT=0
MIROSTAT_TAU=5.0
MIROSTAT_ETA=0.1
PENALIZE_NEWLINE=false
REPEAT_PENALTY=1.1
REPEAT_LAST_N=64
REQUEST_MAX_TOKENS=512
# Optional: NUM_CTX_DEFAULT (wenn gesetzt, wird übernommen)
# NUM_CTX_DEFAULT=4096
```

Hinweise:

- `eval_mode` deckelt `temperature` automatisch auf maximal 0.25.
- `stop` akzeptiert entweder eine Liste von Strings oder einen einzelnen String (wird intern zu einer Liste gewandelt).
- Wertebereiche werden konservativ geprüft/geklammert (z. B. `top_p`, `min_p`, `typical_p`, `tfs_z` in [0,1]; `mirostat` ∈ {0,1,2}).

### Policies aktivieren (optional)

Die Inhalts‑Policies sind standardmäßig aus. Zur Aktivierung in `.env` oder Umgebungsvariablen setzen:

```ini
POLICIES_ENABLED=true
POLICY_FILE="eval/config/policy.sample.json"
# Im "unrestricted"‑Modus strikt alle Policies umgehen:
POLICY_STRICT_UNRESTRICTED_BYPASS=true
```

Hinweise:

- Policy‑Datei kann „default“ und „profiles“ enthalten. Merge‑Reihenfolge: `default` → `profiles[profile_id]`;
   `forbidden_terms` werden vereinigt, `rewrite_map` überlagert die Schlüssel.
- `mode=eval` mappt implizit auf `profile_id="eval"`.
- Details und Tests siehe `docs/AGENT_BEHAVIOR.md` und `tests/test_content_policy_profiles.py`.

Optionale CLI-Tools
-------------------

Für erweiterte Workflows stehen optionale Skripte zur Verfügung (nicht Teil des API-Pflichtpfads):

- `scripts/customize_prompts.py` – Prompts/Policies/Profiles zusammenstellen; Export in Dateien
- `scripts/estimate_tokens.py` – Grobe Token-/Längenabschätzung für Eingaben
- `scripts/open_context_notes.py` – Kontextnotizen aus `settings` öffnen (lokal)
- `scripts/audit_workspace.py` – Konsistenz-/Altlasten-Scan; Hinweise und Pfadprüfungen
- `scripts/openai_finetune.py` – OpenAI-kompatible Finetune-Packs validieren/Triggern
- `scripts/openai_ft_status.py` – Finetune-Status abfragen
- `scripts/train_lora.py` – LoRA-Miniläufe (TinyLlama etc.)
- `scripts/fine_tune_pipeline.py` – End-to-End Pipeline (Export→Prepare→Train)

Tipps:

- Viele Schritte sind als VS Code Tasks vorhanden (Suche nach „Finetune“, „Eval“, „Summary“).
- Alle Skripte akzeptieren `--help` mit Kurzbeschreibung und Argumenten.

Lokales RAG (optional)
----------------------

Der Agent kann optional Kontext‑Snippets aus einem lokalen Text‑Korpus (Markdown/TXT) via leichtgewichtigem TF‑IDF‑Retriever injizieren.

- Flags (in `.env` oder als Umgebungsvariablen):
   - `RAG_ENABLED=true` – RAG aktivieren
   - `RAG_INDEX_PATH=eval/results/rag/index.json` – Pfad zur Index‑Datei
   - `RAG_TOP_K=3` – Anzahl der Snippets

- Indexer‑CLI: `scripts/rag_indexer.py`
   - Baut einen JSON‑Index über `.md`/`.txt` Dateien (nicht rekursiv für Ordner‑Top‑Level)
   - Beispiel (PowerShell):

```powershell
      .\.venv\Scripts\python.exe scripts\rag_indexer.py --input docs eval\config --out eval\results\rag\index.json
      ```

- Verwendung im Server:
   - Server liest `RAG_INDEX_PATH` beim Request ein (best‑effort). Wenn der Index fehlt, läuft der Chat normal weiter (fail‑open).
   - Snippets werden als zusätzliche System‑Nachricht `[RAG]` injiziert.

- Task‑Hinweise:
   - Es gibt aktuell keinen dedizierten VS Code Task für den Indexer; der obige Aufruf funktioniert plattformneutral über den aktiven Interpreter.
   - Optional kann ein eigener Task ergänzt werden, der `scripts/rag_indexer.py` mit gewünschten `--input`/`--out` Werten ausführt.

Datenmodelle (Quelle)
---------------------

Die zentralen Pydantic-Modelle für Requests/Responses liegen in `app/api/models.py`.
Historische `app/schemas.py` wurde entfernt. Bitte nur `app/api/models.py` importieren.

Workspace-Zusammenfassung
--------------------------

- Neueste Gesamt-Zusammenfassung (LLM+Heuristik):
   - eval/results/summaries/summary_ALL_20250824_0306_MIXED.md

Datensatz-Kurierung (3–7 Tage)
------------------------------

Aus Eval-Ergebnissen Trainingspakete erzeugen:

- Skript: `scripts/curate_dataset_from_latest.py`
- Ablauf: nimmt die neueste `results_*.jsonl`, exportiert in `openai_chat` oder `alpaca`, erzeugt deduplizierte Train/Val-Dateien.
- Ausgabe liegt unter `eval/results/finetune/`.

Finetune workflow
-----------------

Schneller Export und Vorbereitung von Trainingspaketen auf Basis der neuesten
Evaluations-Ergebnisse (`eval/results/results_*.jsonl`). Zwei VS Code Tasks sind vorhanden:

- Finetune: export (latest)
   - Ermittelt die neueste `results_*.jsonl` und exportiert nach OpenAI-Chat-Format.
   - Ausgabe: `${workspaceFolder}/eval/results/finetune/exports/openai_chat.jsonl`
   - OS-spezifisch (Windows PowerShell vs. Linux/macOS Bash) hinterlegt.

- Finetune: prepare (split)
   - Erzeugt deduplizierte Splits:
      - Train: `${workspaceFolder}/eval/results/finetune/train.jsonl`
      - Val: `${workspaceFolder}/eval/results/finetune/val.jsonl`
   - Schwellwert für Near-Duplicates: `0.92`

Akzeptanz: Das Ausführen beider Tasks erzeugt valide JSONL-Dateien für Train/Val ohne JSON-Fehler.

Fine-Tuning / LoRA Mini-Pipeline (3–7 Tage)
------------------------------------------

- Skript: `scripts/fine_tune_pipeline.py`
- Voraussetzungen: passende PyTorch-Installation und optionale Pakete aus `requirements-train.txt`.
- Beispiel (CPU/GPU abhängig):
   - python scripts/fine_tune_pipeline.py \
      --finetune-dir eval/results/finetune \
      --epochs 1 \
      --per-device-train-batch-size 1 \
      --bf16

Eval: Synonyme mit privatem Overlay
----------------------------------

Für die Keyword-Checks in der Evaluierung können Synonyme aus `eval/config/synonyms.json` geladen werden.
Zusätzlich können lokale, private Ergänzungen in
`eval/config/synonyms.local.json` abgelegt werden.
Diese Datei ist git-ignoriert und wird automatisch mit der Basisdatei gemerged.

- Beispiel: `eval/config/synonyms.local.sample.json` kopieren zu `synonyms.local.json` und anpassen.

Lokale Kontext-Notizen (optional)
----------------------------------

Der Server kann optionale, lokale Kontext-Notizen als zusätzliche
System-Nachricht injizieren. Das ist nützlich für projektspezifisches Wissen
oder interne Begriffe.

- Beispieldatei: `eval/config/context.local.sample.md` → kopieren zu `context.local.md` und Inhalte ergänzen.
- Aktivierung via Settings/ENV:
   - `CONTEXT_NOTES_ENABLED=true`
   - Optional Pfade anpassen:
      `CONTEXT_NOTES_PATHS=["eval/config/context.local.md", "eval/config/context.local.jsonl", ...]`
   - Optional Größe begrenzen: `CONTEXT_NOTES_MAX_CHARS=4000`
- Die Notizen werden als zweite System-Nachricht eingefügt (nach dem gewählten
   System-Prompt), sowohl im normalen als auch im Streaming-Endpunkt.
- Fehlende Overlay-Datei wird stillschweigend ignoriert.

Eval-Style-Guard (Post-Hook im eval_mode)
----------------------------------------

Der Streaming-Post-Hook normalisiert im `eval_mode` die finale
Assistenten-Antwort heuristisch: neutral, kurz, ohne Rollenspiel/Emoji/
Storytelling. Die Normalisierung greift nur, wenn `eval_mode` aktiv ist und
kann über Settings deaktiviert werden. Der umgeschriebene Text wird in der
Sitzungshistorie persistiert.

- Flags in `app/core/settings.py` (auch per ENV setzbar):
   - `EVAL_POST_REWRITE_ENABLED` (default: `True`)
   - `EVAL_POST_MAX_SENTENCES` (default: `2`)
   - `EVAL_POST_MAX_CHARS` (default: `240`)
   - Heuristiken: Neutralisierung und Kompaktierung
      (Pronomen/Rollenspiel/Emojis/! entfernen, Duplikate/Punktuation
      normalisieren)

- Beispiel: SSE-Tail beim Streaming (eval_mode)

```text
   event: delta
   data: {"text":"..."}
   event: meta
   data: {"policy_post":"rewritten","request_id":"<RID>","delta_len":42}
   event: done
   ```

- Eval-Runner Preset: `--profile eval`
   - Setzt konservative Sampling-Defaults (nur wenn nicht manuell
      überschrieben):
      - `temperature=0.2`, `top_p=0.1`, `max_tokens=128`
   - Checks lassen sich fokussieren, z. B.:
      `--checks rpg_style,term_inclusion`
   - Ruhige Ausgabe: `--quiet`

### Schnelle Rezepte (copy/paste)

- CHAI (ASGI, eval-Profil, fokussierte Checks):

```bash
   python scripts/run_eval.py --asgi --packages "eval/datasets/chai-ai_small_v1.jsonl" \
      --profile eval --checks rpg_style,term_inclusion --quiet
   ```

- Combined 001–100 (ASGI, eval-Profil, fokussierte Checks):

```bash
   python scripts/run_eval.py --asgi --packages "eval/datasets/combined_eval_001-100.jsonl" \
      --profile eval --checks rpg_style,term_inclusion --quiet
   ```

Copilot @workspace / #codebase (Code-Suche)
-------------------------------------------

- Empfehlung: Remote-Index nutzen (Repo liegt auf GitHub). Lokaler Index dient als Fallback.
- Push regelmäßig, damit der Remote-Index aktuell bleibt.
- Nutzung in Prompts: `@workspace` oder `#codebase` hinzufügen, optional Code markieren/auswählen.
- Status und Index-Build über die Copilot-Statusleiste; bei Bedarf "Build Remote Workspace Index" ausführen.

